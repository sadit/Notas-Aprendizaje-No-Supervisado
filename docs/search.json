[
  {
    "objectID": "clustering.html",
    "href": "clustering.html",
    "title": "Agrupamiento",
    "section": "",
    "text": "Los algoritmos de clustering (o agrupamiento) son una clase fundamental de técnicas de aprendizaje no supervisado en el campo del Machine Learning y la Ciencia de Datos. Su propósito principal es organizar un conjunto de datos en grupos de manera que los elementos dentro de un mismo grupo sean más similares entre sí que con respecto a los elementos de otros grupos.",
    "crumbs": [
      "Agrupamiento"
    ]
  },
  {
    "objectID": "clustering.html#el-problema-de-agrupamiento",
    "href": "clustering.html#el-problema-de-agrupamiento",
    "title": "Agrupamiento",
    "section": "El problema de agrupamiento",
    "text": "El problema de agrupamiento\nLos algoritmos de clustering buscan una partición \\(C = \\{C_1, C_2, ..., C_k\\}\\) de un conjunto de datos \\(X = \\{x_1, x_2, ..., x_n\\}\\), donde \\(x_i \\in \\mathbb{R}^d\\), tal que los puntos dentro de cada clúster \\(C_j\\) son similares y los puntos de diferentes clústeres son disímiles. La noción de similitud se define mediante una métrica de distancia (e.g., Euclidiana, Manhattan, Mahalanobis) o una función de similitud (e.g., similitud coseno). Algunos ejemplos prototipicos son los siguientes:\n\nK-Means: Intenta dividir los datos en un número \\(k\\) predefinido de clústeres, asignando cada punto al centroide del clúster más cercano.\nDBSCAN (Density-Based Spatial Clustering of Applications with Noise): Agrupa puntos que están densamente conectados, siendo muy bueno para encontrar clústeres de formas arbitrarias y detectar ruido.\n\nA continuación, se describen algunos algoritmos de clustering.\n\nK-Means (y variantes)\nEs un algoritmo de clustering que busca particionar la base de datos y minimizar la suma de cuadrados de las distancias entre cada punto y el centroide de su clúster asignado. Asume grupos de forma esférica y tamaño similar.\nBasado en el algoritmo de Expectation-Maximization - EM o, algoritmo de Lloyd Wikimedia Foundation, Inc. (s. f.b):\n\nInicialización: Se seleccionan \\(k\\) centroides iniciales aleatoriamente de los puntos de datos o mediante estrategias más sofisticadas, e.g., K-Means++ o k-centers.\nAsignación a centroides cercanos (expectation): Cada punto de datos \\(x_i\\) se asigna al clúster cuyo centroide \\(\\mu_j\\) es el más cercano, según la métrica de distancia seleccionada (comúnmente Euclidiana). La asignación se basa en la función de costo: \\[J = \\sum_{j=1}^{k} \\sum_{x_i \\in C_j} \\|x_i - \\mu_j\\|^2\\]\nActualización de centros (maximization): Los centroides de cada clúster se recalculan como la media de todos los puntos asignados a ese clúster: \\[\\mu_j = \\frac{1}{|C_j|} \\sum_{x_i \\in C_j} x_i\\]\nConvergencia: Los pasos 2 y 3 se repiten iterativamente hasta que la asignación de clústeres ya no cambia, la suma de cuadrados de las distancias se minimiza, o se alcanza un número máximo de iteraciones.\n\nEs rápido y escalable para grandes datasets, fácil de implementar y entender. Sin embargo, se debe tener en cuenta que es sensible a la inicialización de los centroides, requiere que \\(k\\) sea preespecificado, no maneja bien clústeres de formas no esféricas o densidades variables, y es sensible a outliers.\nEjemplo:\n\nfrom IPython.display import Markdown, display\nimport pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.cluster import KMeans\nfrom sklearn.metrics import silhouette_score, adjusted_rand_score\nimport warnings\n\ndf_titanic = sns.load_dataset('titanic')\nfeatures = ['pclass', 'age', 'sibsp', 'parch', 'fare']\ndf_kmeans = df_titanic[features].copy()\ntrue_labels = df_titanic['survived'] # Estas son nuestras \"etiquetas verdaderas\" para el ARI\n\ndf_kmeans['age'].fillna(df_kmeans['age'].median(), inplace=True) # imputación de datos (recordar secciones previas)\n\nK-means es sensible a la escala, por lo que nos aseguramos de normalizar\n\nscaler = StandardScaler()\nscaled_features = scaler.fit_transform(df_kmeans)\n\nAhora, aplicamos K-Means con 2 clusters\n\nn_clusters = 2\nkmeans = KMeans(n_clusters=n_clusters, random_state=42, n_init=10)\nkmeans.fit(scaled_features)\ncluster_labels = kmeans.labels_ # Etiquetas de clúster generadas por K-Means\n\nAhora calculamos las medidas de desempeño\n\nsil = silhouette_score(scaled_features, cluster_labels)\nari_score = adjusted_rand_score(true_labels, cluster_labels)\n\nMarkdown(f\"\"\"\n\nParticiones: {pd.Series(cluster_labels).value_counts().tolist()}\n\n| nombre  |  valor  |\n|---------|---------|\n| coeficiente de silueta | {sil:.3f} |\n| adjusted rand index    | {ari_score:.3f} |\n\"\"\")\n\nParticiones: [643, 248]\n\n\n\nnombre\nvalor\n\n\n\n\ncoeficiente de silueta\n0.376\n\n\nadjusted rand index\n0.102\n\n\n\n\n\n\n\nDBSCAN (Density-Based Spatial Clustering of Applications with Noise)\nIdentifica clústeres como regiones densas de puntos en el espacio de datos, separadas por regiones de menor densidad. Es capaz de descubrir clústeres de formas arbitrarias y es robusto al ruido (outliers).\nSe requieren una serie de hiperpárametros y conceptos para su explicación. Sea \\(\\epsilon\\) el radio máximo para considerar dos puntos como vecinos. MinPts el número mínimo de puntos requeridos para formar una región densa.\n\nSe selecciona un punto de datos aleatorio que no ha sido visitado.\nSe recuperan todos sus vecinos dentro de \\(\\epsilon\\).\nSi el número de vecinos es menor que MinPts, el punto se etiqueta como ruido (potencialmente, hasta que un punto central lo alcance).\nSi el número de vecinos es mayor o igual a MinPts, el punto es un punto central y se inicia un nuevo clúster. Todos sus vecinos dentro de \\(\\epsilon\\) se añaden al clúster (si no están ya asignados a otro clúster o marcados como ruido).\nPara cada nuevo punto añadido al clúster (especialmente si es un punto central), el proceso de expansión continúa recursivamente. Los puntos fronterizos se incluyen en el clúster pero no expanden el clúster por sí mismos.\nEl proceso se repite hasta que todos los puntos han sido visitados.\n\nNo requiere especificar \\(k\\), descubre clústeres de formas arbitrarias, robusto a outliers (los etiqueta como ruido). Es sensible a los parámetros \\(\\epsilon\\) y MinPts (que pueden ser difíciles de elegir, especialmente en datos de alta dimensión), no maneja bien clústeres con densidades muy diferentes. Hay otros algoritmos más modernos como HDBSCAN que pueden ser más robustos a estos problemas.",
    "crumbs": [
      "Agrupamiento"
    ]
  },
  {
    "objectID": "umap.html",
    "href": "umap.html",
    "title": "Reducción de dimensión no lineal – UMAP",
    "section": "",
    "text": "A diferencia de PCA (Análisis de Componentes Principales), las técnicas de reducción de dimensionalidad no lineal pueden capturar relaciones más complejas entre los datos; se suele involucrar una función de distancia o kernel para describir la relación entre dos elementos de una base de datos. Esta función de distancia captura la estructura del espacio mediante una matriz de afinidad o una gráfica de vecinos cercanos, que se cálcula en alta dimensión y se replica en baja baja dimensión. Ver Lee y Verleysen (2007) para un tratado más extenso en el tema.",
    "crumbs": [
      "Reducción de dimensión no lineal -- UMAP"
    ]
  },
  {
    "objectID": "umap.html#uniform-manifold-approximation-and-projection-umap",
    "href": "umap.html#uniform-manifold-approximation-and-projection-umap",
    "title": "Reducción de dimensión no lineal – UMAP",
    "section": "Uniform Manifold Approximation and Projection (UMAP)",
    "text": "Uniform Manifold Approximation and Projection (UMAP)\nUMAP es un algoritmo de reducción de dimensionalidad no lineal que se distingue a su eficiencia computacional, escalabilidad y la capacidad de preservar simultáneamente la estructura local y global de los datos. A diferencia de métodos previos como t-SNE, que prioriza fuertemente la estructura local, UMAP se fundamenta en un marco teórico más riguroso derivado de la geometría Riemanniana y la topología algebraica.\nEl algoritmo de UMAP se puede desglosar en dos fases principales:\n\nConstrucción de una representación topológica fuzzy de alta dimensión:\n\nSe presume que los datos están muestreados a partir de una variedad (manifold) de Riemann de baja dimensionalidad incrustada en un espacio euclidiano de alta dimensionalidad. Se postula que la distribución de los datos en esta variedad es aproximadamente uniforme y que la métrica Riemanniana es localmente constante o puede ser aproximada como tal.\nGrafo de k-vecinos: Para cada punto \\(x_i\\) en el espacio de alta dimensión, se identifican sus \\(k\\) vecinos más cercanos (\\(k\\) es el parámetro n_neighbors).\nComplejo simplicial fuzzy: A partir de estos vecinos, UMAP construye un complejo simplicial fuzzy (un grafo ponderado difuso). En este grafo, los nodos son los puntos de datos y las aristas representan las conectividades. La fuerza de la conexión (peso de la arista) entre dos puntos \\(x_i\\) y \\(x_j\\) se modela como una probabilidad de conectividad \\(p_{ij}\\).\n\nOptimización de la incrustación de baja dimensión:\n\nConstrucción de un grafo de baja dimensión: Se inicializa aleatoriamente una incrustación de los puntos en el espacio de baja dimensión (típicamente 2D o 3D). Sobre esta incrustación, se construye un grafo con conectividades \\(q_{ij}\\) que idealmente reflejan las \\(p_{ij}\\) del espacio de alta dimensión.\nFunción de costo: UMAP minimiza una función de costo de entropía cruzada binaria (o una función de pérdida equivalente) entre las distribuciones de probabilidad de alta y baja dimensión. La función de pérdida penaliza las discordancias:\n\nSi dos puntos están conectados en alta dimensión (\\(p_{ij}\\) es alta) pero no en baja (\\(q_{ij}\\) es baja), hay una penalización fuerte. (Fuerzas atractivas)\nSi dos puntos no están conectados en alta dimensión (\\(p_{ij}\\) es baja) pero sí lo están en baja (\\(q_{ij}\\) es alta), la penalización es menor. (Fuerzas repulsivas, gestionadas eficientemente por un muestreo negativo).\n\nOptimización: La minimización se realiza mediante descenso de gradiente estocástico (SGD). Los puntos en el espacio de baja dimensión se ajustan iterativamente para que las conectividades \\(q_{ij}\\) se acerquen lo más posible a las \\(p_{ij}\\).",
    "crumbs": [
      "Reducción de dimensión no lineal -- UMAP"
    ]
  },
  {
    "objectID": "umap.html#ejemplo",
    "href": "umap.html#ejemplo",
    "title": "Reducción de dimensión no lineal – UMAP",
    "section": "Ejemplo",
    "text": "Ejemplo\n\nfrom IPython.display import Markdown, display\nimport matplotlib.pyplot as plt\nfrom sklearn import datasets\nimport seaborn as sns\nimport pandas as pd\nimport numpy as np\nimport umap \n\nestaremos usando la base de datos digits de digits escritos a mano más alta, en particular de imágenes en escala de grises de \\(8 \\times 8\\), representadas como vectores de 64 dimensiones.\n\ndigits = datasets.load_digits()\nprint(digits.DESCR)\n\n.. _digits_dataset:\n\nOptical recognition of handwritten digits dataset\n--------------------------------------------------\n\n**Data Set Characteristics:**\n\n    :Number of Instances: 1797\n    :Number of Attributes: 64\n    :Attribute Information: 8x8 image of integer pixels in the range 0..16.\n    :Missing Attribute Values: None\n    :Creator: E. Alpaydin (alpaydin '@' boun.edu.tr)\n    :Date: July; 1998\n\nThis is a copy of the test set of the UCI ML hand-written digits datasets\nhttps://archive.ics.uci.edu/ml/datasets/Optical+Recognition+of+Handwritten+Digits\n\nThe data set contains images of hand-written digits: 10 classes where\neach class refers to a digit.\n\nPreprocessing programs made available by NIST were used to extract\nnormalized bitmaps of handwritten digits from a preprinted form. From a\ntotal of 43 people, 30 contributed to the training set and different 13\nto the test set. 32x32 bitmaps are divided into nonoverlapping blocks of\n4x4 and the number of on pixels are counted in each block. This generates\nan input matrix of 8x8 where each element is an integer in the range\n0..16. This reduces dimensionality and gives invariance to small\ndistortions.\n\nFor info on NIST preprocessing routines, see M. D. Garris, J. L. Blue, G.\nT. Candela, D. L. Dimmick, J. Geist, P. J. Grother, S. A. Janet, and C.\nL. Wilson, NIST Form-Based Handprint Recognition System, NISTIR 5469,\n1994.\n\n|details-start|\n**References**\n|details-split|\n\n- C. Kaynak (1995) Methods of Combining Multiple Classifiers and Their\n  Applications to Handwritten Digit Recognition, MSc Thesis, Institute of\n  Graduate Studies in Science and Engineering, Bogazici University.\n- E. Alpaydin, C. Kaynak (1998) Cascading Classifiers, Kybernetika.\n- Ken Tang and Ponnuthurai N. Suganthan and Xi Yao and A. Kai Qin.\n  Linear dimensionalityreduction using relevance weighted LDA. School of\n  Electrical and Electronic Engineering Nanyang Technological University.\n  2005.\n- Claudio Gentile. A New Approximate Maximal Margin Classification\n  Algorithm. NIPS. 2000.\n\n|details-end|\n\n\nEl modelo UMAP se aprende y se usa para transformar la base de datos original de 784 dimensiones a solo dos. Las imagenes se ven como sigue:\n\n_, axes = plt.subplots(nrows=1, ncols=10, figsize=(10, 3))\n\nfor ax, image, label in zip(axes, digits.images, digits.target):\n    ax.set_axis_off()\n    ax.imshow(image, cmap=plt.cm.gray_r, interpolation=\"nearest\")\n    ax.set_title(\"label: %i\" % label)\n\nplt.show()\n#plt.matshow(digits.images[0])\n\n\n\n\n\n\n\n\nDefinición del modelo UMAP y transformación:\n\nemb = umap.UMAP(metric=\"euclidean\", n_neighbors=15, n_components=2, n_jobs=16).fit_transform(digits.data)\n\nGraficamos con seaborn:\n\ndf = pd.DataFrame(data=emb, columns=[\"x\", \"y\"])\ndf[\"target\"] = digits.target\nsns.scatterplot(x=\"x\", y=\"y\",\n    hue=\"target\", palette='viridis',\n    alpha=0.7, data=df)\n\n\n\n\n\n\n\n\n\n¿Cómo se vería con PCA 2D?\nEsta pregunta se responderá comparando de manera práctica\n\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.decomposition import PCA\n\nscaler = StandardScaler()\nscaled_features = scaler.fit_transform(digits.data)\n#scaled_features = digits.data\nmodel = PCA(n_components=2)\nprincipal_components = model.fit_transform(scaled_features)\npca_df = pd.DataFrame(data=principal_components, columns=['PC1', 'PC2'])\npca_df['target'] = digits['target']\nsns.scatterplot(\n    data=pca_df,\n    x='PC1',\n    y='PC2',\n    hue='target', # Colorea los puntos según si sobrevivieron o no\n    palette='viridis', # Esquema de colores\n    alpha=0.7 # Transparencia\n)\n\n\n\n\n\n\n\n\n\n\nParametros: número de vecinos\n\nfor nn in [3, 10, 15, 30, 50, 100, 300, 1000, 1797]:\n    emb = umap.UMAP(metric=\"euclidean\", n_neighbors=nn, n_components=2, n_jobs=16).fit_transform(digits.data)\n    display(Markdown(f\"## n_neighbors={nn}\"))\n    df = pd.DataFrame(data=emb, columns=[\"x\", \"y\"])\n    df[\"target\"] = digits.target\n    sns.scatterplot(x=\"x\", y=\"y\",\n        hue=\"target\", palette='viridis',\n        alpha=0.7, data=df)\n    plt.title(f\"n_neighbors={nn}\")\n    plt.show()\n\n/home/sadit/miniconda3/lib/python3.11/site-packages/sklearn/manifold/_spectral_embedding.py:273: UserWarning: Graph is not fully connected, spectral embedding may not work as expected.\n  warnings.warn(\n\n\nn_neighbors=3\n\n\n\n\n\n\n\n\n\nn_neighbors=10\n\n\n\n\n\n\n\n\n\nn_neighbors=15\n\n\n\n\n\n\n\n\n\nn_neighbors=30\n\n\n\n\n\n\n\n\n\nn_neighbors=50\n\n\n\n\n\n\n\n\n\nn_neighbors=100\n\n\n\n\n\n\n\n\n\nn_neighbors=300\n\n\n\n\n\n\n\n\n\nn_neighbors=1000\n\n\n\n\n\n\n\n\n\n/home/sadit/miniconda3/lib/python3.11/site-packages/umap/umap_.py:2462: UserWarning: n_neighbors is larger than the dataset size; truncating to X.shape[0] - 1\n  warn(\n\n\nn_neighbors=1797\n\n\n\n\n\n\n\n\n\nAhora veremos como actua el parámetro min_dist\n\nfor min_dist in [0.01, 0.03, 0.1, 0.3, 1.0]:\n    nn = 15\n    emb = umap.UMAP(metric=\"euclidean\", n_neighbors=nn, n_components=2, n_jobs=16, min_dist=min_dist).fit_transform(digits.data)\n    display(Markdown(f\"## n_neighbors={nn}, min_dist={min_dist}\"))\n    df = pd.DataFrame(data=emb, columns=[\"x\", \"y\"])\n    df[\"target\"] = digits.target\n    sns.scatterplot(x=\"x\", y=\"y\",\n        hue=\"target\", palette='viridis',\n        alpha=0.7, data=df)\n    plt.title(f\"n_neighbors={nn}\")\n    plt.show()\n\nn_neighbors=15, min_dist=0.01\n\n\n\n\n\n\n\n\n\nn_neighbors=15, min_dist=0.03\n\n\n\n\n\n\n\n\n\nn_neighbors=15, min_dist=0.1\n\n\n\n\n\n\n\n\n\nn_neighbors=15, min_dist=0.3\n\n\n\n\n\n\n\n\n\nn_neighbors=15, min_dist=1.0",
    "crumbs": [
      "Reducción de dimensión no lineal -- UMAP"
    ]
  },
  {
    "objectID": "umap.html#n_neighbors3",
    "href": "umap.html#n_neighbors3",
    "title": "Reducción de dimensión no lineal – UMAP",
    "section": "n_neighbors=3",
    "text": "n_neighbors=3",
    "crumbs": [
      "Reducción de dimensión no lineal -- UMAP"
    ]
  },
  {
    "objectID": "umap.html#n_neighbors10",
    "href": "umap.html#n_neighbors10",
    "title": "Reducción de dimensión no lineal – UMAP",
    "section": "n_neighbors=10",
    "text": "n_neighbors=10",
    "crumbs": [
      "Reducción de dimensión no lineal -- UMAP"
    ]
  },
  {
    "objectID": "umap.html#n_neighbors15",
    "href": "umap.html#n_neighbors15",
    "title": "Reducción de dimensión no lineal – UMAP",
    "section": "n_neighbors=15",
    "text": "n_neighbors=15",
    "crumbs": [
      "Reducción de dimensión no lineal -- UMAP"
    ]
  },
  {
    "objectID": "umap.html#n_neighbors30",
    "href": "umap.html#n_neighbors30",
    "title": "Reducción de dimensión no lineal – UMAP",
    "section": "n_neighbors=30",
    "text": "n_neighbors=30",
    "crumbs": [
      "Reducción de dimensión no lineal -- UMAP"
    ]
  },
  {
    "objectID": "umap.html#n_neighbors50",
    "href": "umap.html#n_neighbors50",
    "title": "Reducción de dimensión no lineal – UMAP",
    "section": "n_neighbors=50",
    "text": "n_neighbors=50",
    "crumbs": [
      "Reducción de dimensión no lineal -- UMAP"
    ]
  },
  {
    "objectID": "umap.html#n_neighbors100",
    "href": "umap.html#n_neighbors100",
    "title": "Reducción de dimensión no lineal – UMAP",
    "section": "n_neighbors=100",
    "text": "n_neighbors=100",
    "crumbs": [
      "Reducción de dimensión no lineal -- UMAP"
    ]
  },
  {
    "objectID": "umap.html#n_neighbors300",
    "href": "umap.html#n_neighbors300",
    "title": "Reducción de dimensión no lineal – UMAP",
    "section": "n_neighbors=300",
    "text": "n_neighbors=300",
    "crumbs": [
      "Reducción de dimensión no lineal -- UMAP"
    ]
  },
  {
    "objectID": "umap.html#n_neighbors1000",
    "href": "umap.html#n_neighbors1000",
    "title": "Reducción de dimensión no lineal – UMAP",
    "section": "n_neighbors=1000",
    "text": "n_neighbors=1000",
    "crumbs": [
      "Reducción de dimensión no lineal -- UMAP"
    ]
  },
  {
    "objectID": "umap.html#n_neighbors1797",
    "href": "umap.html#n_neighbors1797",
    "title": "Reducción de dimensión no lineal – UMAP",
    "section": "n_neighbors=1797",
    "text": "n_neighbors=1797",
    "crumbs": [
      "Reducción de dimensión no lineal -- UMAP"
    ]
  },
  {
    "objectID": "umap.html#n_neighbors15-min_dist0.01",
    "href": "umap.html#n_neighbors15-min_dist0.01",
    "title": "Reducción de dimensión no lineal – UMAP",
    "section": "n_neighbors=15, min_dist=0.01",
    "text": "n_neighbors=15, min_dist=0.01",
    "crumbs": [
      "Reducción de dimensión no lineal -- UMAP"
    ]
  },
  {
    "objectID": "umap.html#n_neighbors15-min_dist0.03",
    "href": "umap.html#n_neighbors15-min_dist0.03",
    "title": "Reducción de dimensión no lineal – UMAP",
    "section": "n_neighbors=15, min_dist=0.03",
    "text": "n_neighbors=15, min_dist=0.03",
    "crumbs": [
      "Reducción de dimensión no lineal -- UMAP"
    ]
  },
  {
    "objectID": "umap.html#n_neighbors15-min_dist0.1",
    "href": "umap.html#n_neighbors15-min_dist0.1",
    "title": "Reducción de dimensión no lineal – UMAP",
    "section": "n_neighbors=15, min_dist=0.1",
    "text": "n_neighbors=15, min_dist=0.1",
    "crumbs": [
      "Reducción de dimensión no lineal -- UMAP"
    ]
  },
  {
    "objectID": "umap.html#n_neighbors15-min_dist0.3",
    "href": "umap.html#n_neighbors15-min_dist0.3",
    "title": "Reducción de dimensión no lineal – UMAP",
    "section": "n_neighbors=15, min_dist=0.3",
    "text": "n_neighbors=15, min_dist=0.3",
    "crumbs": [
      "Reducción de dimensión no lineal -- UMAP"
    ]
  },
  {
    "objectID": "umap.html#n_neighbors15-min_dist1.0",
    "href": "umap.html#n_neighbors15-min_dist1.0",
    "title": "Reducción de dimensión no lineal – UMAP",
    "section": "n_neighbors=15, min_dist=1.0",
    "text": "n_neighbors=15, min_dist=1.0",
    "crumbs": [
      "Reducción de dimensión no lineal -- UMAP"
    ]
  },
  {
    "objectID": "umap.html#criterios-prácticos-para-elegir-entre-pca-y-umap",
    "href": "umap.html#criterios-prácticos-para-elegir-entre-pca-y-umap",
    "title": "Reducción de dimensión no lineal – UMAP",
    "section": "Criterios prácticos para elegir entre PCA y UMAP",
    "text": "Criterios prácticos para elegir entre PCA y UMAP\n\nEstructura\nEficiencia\nReproducibilidad",
    "crumbs": [
      "Reducción de dimensión no lineal -- UMAP"
    ]
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Aprendizaje no supervisado y visualización",
    "section": "",
    "text": "En el estudio de la Inteligencia Artificial (IA), el aprendizaje no supervisado y la visualización de datos son pilares fundamentales. El aprendizaje no supervisado es crucial porque le permite a la IA descubrir patrones y estructuras ocultas en datos que no han sido previamente etiquetados o categorizados.\nA diferencia del aprendizaje supervisado, que necesita ejemplos de entrada con sus respuestas correctas, el aprendizaje no supervisado trabaja con datos crudos, lo cual es una gran ventaja ya que la mayoría de los datos disponibles en el mundo real no están etiquetados.",
    "crumbs": [
      "Inicio"
    ]
  },
  {
    "objectID": "index.html#manos-a-la-obra",
    "href": "index.html#manos-a-la-obra",
    "title": "Aprendizaje no supervisado y visualización",
    "section": "Manos a la obra",
    "text": "Manos a la obra\nPara esta Unidad se requieren algunos paquetes de Python, Para esta Unidad se necesita una instalación de Python 3, en particular se probó en la versión 3.11. Para las notas se utilizó miniconda, pero si te es posible, utiliza la versión completa Anaconda https://www.anaconda.com/. Se utilizan varios paquetes, los cuales se pueden instalar como sigue:\npip install pandas matplotlib seaborn scikit-learn umap-learn nndescent hdbscan\nPara esto se necesita acceso a internet. También se necesitan algunas bases de datos pequeñas que se pueden bajar automáticamente de internet.\nTambién se recomienda usar Visual Studio Code https://code.visualstudio.com/ para trabajar con el código. Se puede usar en diferentes plataformas, tiene plugins para jupyter y para quarto, así como también puede ejecutar código directamente de archivos.\nSi se prefiere, se puede utilizar jupyter o quarto sin vscode.\nEn todo caso será necesario instalar jupyter y/o quarto; se recomienda seguir las instrucciones de los sitios oficiales\n\nhttps://jupyter.org/install\nhttps://quarto.org/",
    "crumbs": [
      "Inicio"
    ]
  },
  {
    "objectID": "vis.html",
    "href": "vis.html",
    "title": "Uso de paquetería para ploteo",
    "section": "",
    "text": "Una parte importante del análisis es la gráficación de resultados, nos permite obtener información visual sobre relaciones que nos permitirán comprender información de un problema.\nEn python podemos encontrar una múltitud de paquetes nos ayudan a graficar resultados, con diversas recetas diseñadas para mejorar la apreciación de los mismos en diferentes dominios de aplicación.\nEn este curso revisaremos dos paquetes:\nSitios recomendatos para aprender sobre matplotlib y seaborn:",
    "crumbs": [
      "Uso de paquetería para ploteo"
    ]
  },
  {
    "objectID": "vis.html#creación-de-gráficas",
    "href": "vis.html#creación-de-gráficas",
    "title": "Uso de paquetería para ploteo",
    "section": "Creación de gráficas",
    "text": "Creación de gráficas\nLa figura Figura 1 muestra una gráfica de una curva. Observese el código, en particular como se indican los argumentos\n\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nplt.plot(np.arange(0, 10, 0.1)**2)\nplt.ylabel('una curva')\nplt.xlabel('x')\nplt.legend('x**2')\nplt.show()\n\n\n\n\n\n\n\nFigura 1: Una función cuadrada con etiquetas a los lados\n\n\n\n\n\nEs posible notar que se le dieron los valores en el eje \\(y\\), y que infirió los valores en \\(x\\); esto es cómodo, aunque muchas veces deberíamos escoger lo forma más prolija pero precisa de Figura 2:\n\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nx = np.arange(0, 10, 0.1)\nplt.plot(x, x**2)\nplt.ylabel('una curva')\nplt.xlabel('x')\nplt.legend('x**2')\nplt.show()\n\n\n\n\n\n\n\nFigura 2: Una función cuadrada con etiquetas a los lados (v2)\n\n\n\n\n\nNote como el rango de la función ahora coincide con los datos que la originaron.\nMuchas veces es buena idea tener un buen balance entre expresividad y cantidad de código, como en Figura 3\n\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nx = np.arange(0, 10, 0.1)\nplt.plot(x, x, 'g-',  x, x**2, 'r--', x, x**3, 'b^')\nplt.ylabel('curvas')\nplt.xlabel('x')\nplt.title('diferentes ordenes de crecimiento')\nplt.show()\n\n\n\n\n\n\n\nFigura 3: Definición compacta de varias curvas\n\n\n\n\n\neste es un estilo compacto heredado de MATLAB, pero no hay que preocuparse de lo intrincado de la definición y descripción de lo que se le solicta a pyplot, ya que cuenta con una extensa documentación que puede ser consultada para ver muchos más detalles.\nHasta ahora, hemos visto los métodos plot y scatter; pero también otras funciones muy útiles como es el caso de los histogramas, ver Figura 4.\n\nimport numpy as np\nimport matplotlib.pyplot as plt\n\ndata = np.random.randn(5000)\n\n# the histogram of the data\nplt.hist(data, 20, density=True, alpha=0.5)\n\nplt.xlabel('x')\nplt.ylabel('prob')\nplt.axis([-4, 4, 0, 0.5])\nplt.grid(True)\nplt.show()\n\n\n\n\n\n\n\nFigura 4: Histograma\n\n\n\n\n\nMuchas veces es necesario separar las figuras en dos o más gráficas, esto se logra de manera explícita controlado el layout como se muestra en Figura 5. Este ejemplo fue tomado del tutorial de pyplot.\n\nimport numpy as np\nimport matplotlib.pyplot as plt\n\ndef f(t):\n    return np.cos(2*np.pi*t)\n\nt1 = np.arange(0.0, 5.0, 0.1)\nt2 = np.arange(0.0, 5.0, 0.02)\n\nplt.figure()\nplt.subplot(211)\nplt.plot(t1, f(t1), 'bo', t2, f(t2), 'k')\n\nplt.subplot(212)\nplt.plot(t2, np.cos(2*np.pi*t2), 'r--')\nplt.show()\n\n\n\n\n\n\n\nFigura 5: Gráfica con sub-figuras\n\n\n\n\n\nNotesé como se índican los múltiples subplots, el número codifica un layout de malla, i.e., número de filas, número de columnas, e índice del subplot (contando de izquierda a derecha y de arriba a abajo).",
    "crumbs": [
      "Uso de paquetería para ploteo"
    ]
  },
  {
    "objectID": "vis.html#gráficando-datos-en-forma-de-tabla",
    "href": "vis.html#gráficando-datos-en-forma-de-tabla",
    "title": "Uso de paquetería para ploteo",
    "section": "Gráficando datos en forma de tabla",
    "text": "Gráficando datos en forma de tabla\nCuando se tienen datos relacionados a manera de tabla, es posible usar una versión del API que simplifica aún más la creación de figuras:\n\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nx = np.arange(0, 10, 0.2)\ndata = {\n    'x': x,\n    'cuadrada': x**2,\n    'color': np.random.randint(0, 10, len(x)),\n    'area': (np.sin(x) + 1.1) * 15,\n}\n\n\nplt.scatter('x', 'cuadrada', c='color', s='area', data=data)\nplt.ylabel('$x^2$')\nplt.xlabel('$x$')\nplt.show()\n\n\n\n\n\n\n\nFigura 6: Las estructuras de datos tipo tabla pueden usarse simplificando las gráficas\n\n\n\n\n\nUna manera más integrada de usar usar tablas es seaborn``. @fig-sb-tips muestra un ejemplo del uso deseaborncon la tablatips`, incluida en ese mismo paquete.\n\nimport numpy as np\nimport seaborn as sns\n\nsns.set_theme()\ntips = sns.load_dataset(\"tips\")\nprint(tips)\n\nsns.relplot(\n    data=tips,\n    x=\"total_bill\", y=\"tip\", col=\"time\", \n    hue=\"smoker\", style=\"smoker\", size=\"size\",\n)\n\n     total_bill   tip     sex smoker   day    time  size\n0         16.99  1.01  Female     No   Sun  Dinner     2\n1         10.34  1.66    Male     No   Sun  Dinner     3\n2         21.01  3.50    Male     No   Sun  Dinner     3\n3         23.68  3.31    Male     No   Sun  Dinner     2\n4         24.59  3.61  Female     No   Sun  Dinner     4\n..          ...   ...     ...    ...   ...     ...   ...\n239       29.03  5.92    Male     No   Sat  Dinner     3\n240       27.18  2.00  Female    Yes   Sat  Dinner     2\n241       22.67  2.00    Male    Yes   Sat  Dinner     2\n242       17.82  1.75    Male     No   Sat  Dinner     2\n243       18.78  3.00  Female     No  Thur  Dinner     2\n\n[244 rows x 7 columns]\n\n\n\n\n\n\n\n\nFigura 7: Las estructuras de datos tipo tabla pueden usarse simplificando las gráficas\n\n\n\n\n\nrelplot permite definir una malla de subfiguras fácilmente usando una columna como indicador de columna y/o filas.\nEl siguiente ejemplo es un poco más elaborado y usa la base de datos de titanic:\n\nimport pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\ndf_titanic = sns.load_dataset('titanic')\n\ndf_titanic.head()\n\n\n\n\n\n\n\n\nsurvived\npclass\nsex\nage\nsibsp\nparch\nfare\nembarked\nclass\nwho\nadult_male\ndeck\nembark_town\nalive\nalone\n\n\n\n\n0\n0\n3\nmale\n22.0\n1\n0\n7.2500\nS\nThird\nman\nTrue\nNaN\nSouthampton\nno\nFalse\n\n\n1\n1\n1\nfemale\n38.0\n1\n0\n71.2833\nC\nFirst\nwoman\nFalse\nC\nCherbourg\nyes\nFalse\n\n\n2\n1\n3\nfemale\n26.0\n0\n0\n7.9250\nS\nThird\nwoman\nFalse\nNaN\nSouthampton\nyes\nTrue\n\n\n3\n1\n1\nfemale\n35.0\n1\n0\n53.1000\nS\nFirst\nwoman\nFalse\nC\nSouthampton\nyes\nFalse\n\n\n4\n0\n3\nmale\n35.0\n0\n0\n8.0500\nS\nThird\nman\nTrue\nNaN\nSouthampton\nno\nTrue\n\n\n\n\n\n\n\n\ndf_titanic.info()\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 891 entries, 0 to 890\nData columns (total 15 columns):\n #   Column       Non-Null Count  Dtype   \n---  ------       --------------  -----   \n 0   survived     891 non-null    int64   \n 1   pclass       891 non-null    int64   \n 2   sex          891 non-null    object  \n 3   age          714 non-null    float64 \n 4   sibsp        891 non-null    int64   \n 5   parch        891 non-null    int64   \n 6   fare         891 non-null    float64 \n 7   embarked     889 non-null    object  \n 8   class        891 non-null    category\n 9   who          891 non-null    object  \n 10  adult_male   891 non-null    bool    \n 11  deck         203 non-null    category\n 12  embark_town  889 non-null    object  \n 13  alive        891 non-null    object  \n 14  alone        891 non-null    bool    \ndtypes: bool(2), category(2), float64(2), int64(4), object(5)\nmemory usage: 80.7+ KB\n\n\ngráfica: relación entre edad, tarifa y supervivencia\n\nplt.figure(figsize=(10, 6)) \nsns.scatterplot(data=df_titanic, x='age', y='fare', hue='survived', alpha=0.6)\nplt.title('Edad, tarifa y supervivencia')\nplt.xlabel('Edad')\nplt.ylabel('Tarifa pagada')\nplt.legend(title='¿Quienes sobrevivieron?')\nplt.grid(True, linestyle='--', alpha=0.7)\nplt.show()\n\n\n\n\n\n\n\n\nAhora, veremos la distribución de edad por tipo de supervivencia\n\nplt.figure(figsize=(10, 6))\nsns.histplot(data=df_titanic, x='age', hue='survived', multiple='stack', bins=20, kde=True)\nplt.title('Distribución de Edad')\nplt.xlabel('Edad')\nplt.ylabel('Número de pasajeros')\nplt.legend(title='¿Sobrevivió?', labels=['No', 'Sí'])\nplt.grid(True, linestyle='--', alpha=0.7)\nplt.show()\n\n\n\n\n\n\n\n\nOtro ejemplo: ¿La clase influyó en la supervivencia? Gráfico de barras de supervivencia por clase de pasajero\n\n# --- \nplt.figure(figsize=(8, 5))\nsns.barplot(data=df_titanic, x='pclass', y='survived') # ci=None para no mostrar barras de error\nplt.title('Tasa de supervivencia por clase de pasajero')\nplt.xlabel('Clase de Pasajero')\nplt.ylabel('Tasa de Supervivencia')\nplt.xticks(ticks=[0, 1, 2], labels=['Primera', 'Segunda', 'Tercera'])\nplt.grid(axis='y', linestyle='--', alpha=0.7)\nplt.show()",
    "crumbs": [
      "Uso de paquetería para ploteo"
    ]
  },
  {
    "objectID": "pca.html",
    "href": "pca.html",
    "title": "Reducción de dimensionalidad – uso de componentes principales",
    "section": "",
    "text": "La reducción de dimensionalidad es una técnica fundamental en el campo del aprendizaje automático y la ciencia de datos, cuyo objetivo principal es disminuir el número de características (o dimensiones) en un conjunto de datos sin perder la información más relevante o significativa.",
    "crumbs": [
      "Reducción de dimensionalidad -- uso de componentes principales"
    ]
  },
  {
    "objectID": "pca.html#análisis-de-componentes-principales-pca",
    "href": "pca.html#análisis-de-componentes-principales-pca",
    "title": "Reducción de dimensionalidad – uso de componentes principales",
    "section": "Análisis de Componentes Principales (PCA)",
    "text": "Análisis de Componentes Principales (PCA)\nPCA transforma un conjunto de datos con múltiples variables correlacionadas en un nuevo conjunto de variables no correlacionadas, denominadas componentes principales, que retienen la mayor parte de la varianza (información) del conjunto de datos original, pero en un espacio de menor dimensión.\n\nProcedimiento\nConceptualmente, PCA busca identificar las direcciones de máxima variabilidad en los datos. Operacionalmente, el proceso se desglosa en los siguientes pasos:\n\nEstandarización de las componentes: Dada la sensibilidad de PCA a la escala de las variables, es imperativo estandarizar los datos. Esto implica transformar cada variable para que tenga una media de cero y una desviación estándar de uno. Este paso asegura que las variables con mayores rangos de valores no dominen indebidamente el cálculo de las componentes.\nCálculo de la matriz de covarianza (o correlación): Se construye la matriz de covarianza (o la matriz de correlación, si los datos fueron previamente estandarizados), la cual cuantifica la relación lineal entre cada par de variables. Esta matriz es crucial porque la varianza y las covarianzas son los insumos para determinar las direcciones de mayor variabilidad.\nComputación de vectores y valores propios (eigenvectores y eigenvalores):\n\nEigenvectores: Representan las direcciones (ejes) de máxima varianza en el espacio de los datos. Estos eigenvectores son las componentes principales. Son ortogonales entre sí, lo que asegura que cada componente capture una dimensión de variabilidad independiente de las demás.\nEigenvalores: Cada eigenvector está asociado a un eigenvalor, que cuantifica la cantidad de varianza explicada por la componente principal correspondiente. Un eigenvalor más grande indica que la componente asociada captura una mayor proporción de la varianza total de los datos.\n\nSelección de los componentes Principales: Los eigenvectores se ordenan descendentemente según sus autovalores. La primera componente principal es la que explica la mayor varianza, la segunda la siguiente mayor varianza, y así sucesivamente.\nProyección: Finalmente, los datos originales se proyectan sobre el subespacio definido por las componentes principales seleccionadas. Esta transformación genera un nuevo conjunto de datos de menor dimensionalidad, donde cada nueva variable (componente principal) es una combinación lineal de las características originales.\n\n\n\nAplicaciones de PCA\n\nReducción de dimensionalidad.\nVisualización de datos de alta dimensionalidad.\nReducción de ruido.\n\n\n\nLimitaciones\n\nEs un método lineal.\nInterpretación.\nSensibilidad a la escala.",
    "crumbs": [
      "Reducción de dimensionalidad -- uso de componentes principales"
    ]
  },
  {
    "objectID": "pca.html#ejemplos",
    "href": "pca.html#ejemplos",
    "title": "Reducción de dimensionalidad – uso de componentes principales",
    "section": "Ejemplos",
    "text": "Ejemplos\n\nfrom IPython.display import Markdown\nimport pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.decomposition import PCA\nimport numpy as np\n\ndf_titanic = sns.load_dataset('titanic')\n\ndf_titanic.head(10)\n\n\n\n\n\n\n\n\nsurvived\npclass\nsex\nage\nsibsp\nparch\nfare\nembarked\nclass\nwho\nadult_male\ndeck\nembark_town\nalive\nalone\n\n\n\n\n0\n0\n3\nmale\n22.0\n1\n0\n7.2500\nS\nThird\nman\nTrue\nNaN\nSouthampton\nno\nFalse\n\n\n1\n1\n1\nfemale\n38.0\n1\n0\n71.2833\nC\nFirst\nwoman\nFalse\nC\nCherbourg\nyes\nFalse\n\n\n2\n1\n3\nfemale\n26.0\n0\n0\n7.9250\nS\nThird\nwoman\nFalse\nNaN\nSouthampton\nyes\nTrue\n\n\n3\n1\n1\nfemale\n35.0\n1\n0\n53.1000\nS\nFirst\nwoman\nFalse\nC\nSouthampton\nyes\nFalse\n\n\n4\n0\n3\nmale\n35.0\n0\n0\n8.0500\nS\nThird\nman\nTrue\nNaN\nSouthampton\nno\nTrue\n\n\n5\n0\n3\nmale\nNaN\n0\n0\n8.4583\nQ\nThird\nman\nTrue\nNaN\nQueenstown\nno\nTrue\n\n\n6\n0\n1\nmale\n54.0\n0\n0\n51.8625\nS\nFirst\nman\nTrue\nE\nSouthampton\nno\nTrue\n\n\n7\n0\n3\nmale\n2.0\n3\n1\n21.0750\nS\nThird\nchild\nFalse\nNaN\nSouthampton\nno\nFalse\n\n\n8\n1\n3\nfemale\n27.0\n0\n2\n11.1333\nS\nThird\nwoman\nFalse\nNaN\nSouthampton\nyes\nFalse\n\n\n9\n1\n2\nfemale\n14.0\n1\n0\n30.0708\nC\nSecond\nchild\nFalse\nNaN\nCherbourg\nyes\nFalse\n\n\n\n\n\n\n\nYa cargamos la base de datos, ahora seleccionamos algunas de las variables numéricas para crear el modelo PCA:\n\npclass: clase del pasaje\nage: edad\nsibsp: hermanos/cónyuges a bordo\nparch: padres/hijos a bordo\nfare: tarifa pagada\n\nRecuerda que debemos estandarizar las variables, imputar si es necesario\n\ndf_titanic.isnull().sum()\n\nsurvived         0\npclass           0\nsex              0\nage            177\nsibsp            0\nparch            0\nfare             0\nembarked         2\nclass            0\nwho              0\nadult_male       0\ndeck           688\nembark_town      2\nalive            0\nalone            0\ndtype: int64\n\n\nahora sí\n\nF = ['pclass', 'age', 'sibsp', 'parch', 'fare']\ndf_pca = df_titanic[F].copy()\ndf_pca['age'].fillna(df_pca['age'].median(), inplace=True)\n\nCómo PCA es sensible a la escala, es crucial estandarizar los datos.\n\nscaler = StandardScaler()\nscaled_features = scaler.fit_transform(df_pca)\n\nCrearemos un modelo de PCA para proyectar a 2 dimensiones, excelente para plotear\n\nmodel = PCA(n_components=2)\nprincipal_components = model.fit_transform(scaled_features)\n\npca_df = pd.DataFrame(data=principal_components, columns=['PC1', 'PC2'])\n\n# Añadir la columna de supervivencia para la visualización\npca_df['survived'] = df_titanic['survived']\n\npca_df.head(5)\n\n\n\n\n\n\n\n\nPC1\nPC2\nsurvived\n\n\n\n\n0\n-1.140660\n-0.076520\n0\n\n\n1\n1.768445\n0.133479\n1\n\n\n2\n-0.879196\n-0.726606\n1\n\n\n3\n1.458103\n0.071555\n1\n\n\n4\n-0.561252\n-0.943713\n0\n\n\n\n\n\n\n\nAhora si podemos plotear en 2D la proyección de las 5 componentes originales\n\nplt.figure(figsize=(10, 8))\nsns.scatterplot(\n    data=pca_df,\n    x='PC1',\n    y='PC2',\n    hue='survived', # Colorea los puntos según si sobrevivieron o no\n    palette='viridis', # Esquema de colores\n    alpha=0.7 # Transparencia\n)\nplt.title('Proyección 2D con PCA para la base de datos de titanic')\nplt.xlabel(f'Componente Principal 1 ({model.explained_variance_ratio_[0]*100:.2f}% de varianza)')\nplt.ylabel(f'Componente Principal 2 ({model.explained_variance_ratio_[1]*100:.2f}% de varianza)')\nplt.legend(title='Sobrevivió', labels=['No', 'Sí'])\nplt.grid(True, linestyle='--', alpha=0.6)\nplt.show()\n\n\n\n\n\n\n\nFigura 1\n\n\n\n\n\nSi el objetivo no es visualizar, conocel la varianza acumulada es muy útil para decidir cuántas componentes se deben utilizar; claramente, esta selección tiene implicaciones en la fidelidad que se tendrá entre los datos originales y el modelo PCA, y suele estar guiado por consideraciones prácticas.\n\nmodel = PCA(n_components=5)\nprincipal_components = model.fit_transform(scaled_features)\n\nplt.figure(figsize=(8, 5))\nplt.plot(range(1, len(model.explained_variance_ratio_) + 1), model.explained_variance_ratio_.cumsum(), marker='o', linestyle='--')\nplt.title('Varianza acumulada explicada por componentes principales')\nplt.xlabel('Número de componentes principales')\nplt.ylabel('Varianza acumulada explicada')\nplt.grid(True, linestyle='--', alpha=0.7)\nplt.show()\n\n\n\n\n\n\n\nFigura 2",
    "crumbs": [
      "Reducción de dimensionalidad -- uso de componentes principales"
    ]
  },
  {
    "objectID": "Interactive-1.html",
    "href": "Interactive-1.html",
    "title": "Aprendizaje no supervisado y visualización",
    "section": "",
    "text": "Connected to base (Python 3.11.4)\n\nfrom sklearn import datasets\nimport seaborn as sns\nimport pandas as pd\nimport numpy as np\nimport umap \n\n\ndigits = datasets.load_digits()\n\n\nmodel = umap.UMAP(metric=\"euclidean\", n_neighbors=15, n_jobs=16).fit(digits.data)\n\n\nmodel.transform()\n\n\n---------------------------------------------------------------------------\nTypeError                                 Traceback (most recent call last)\nCell In[4], line 1\n----&gt; 1 model.transform()\n\nTypeError: UMAP.transform() missing 1 required positional argument: 'X'\n\n\n\n\nmodel.transform(digits.data)\n\narray([[15.841952  ,  5.0676193 ],\n       [ 0.46377385, 11.460215  ],\n       [ 2.6353385 ,  8.794771  ],\n       ...,\n       [-0.98420984,  9.280262  ],\n       [-5.408144  ,  4.191199  ],\n       [-1.3129224 ,  8.704093  ]], dtype=float32)\n\n\n\nemb = umap.UMAP(metric=\"euclidean\", n_neighbors=15, n_jobs=16).fit_transform(digits.data)\n\n\ndf = pd.DataFrame(data=emb)\n#model.plot.points(model, labels=digits.target, width=400, height=400)\n\n\ndf = pd.DataFrame(data=emb, columns=[\"x\", \"y\"])\ndf[\"target\"] = digits.values\nsns.scatterplot(x=\"x\", y=\"y\", c=\"target\")\n#model.plot.points(model, labels=digits.target, width=400, height=400)\n\n\n---------------------------------------------------------------------------\nValueError                                Traceback (most recent call last)\nFile /home/sadit/Cursos/MEIA2025/umap.qmd:3\n      1 df = pd.DataFrame(data=emb, columns=[\"x\", \"y\"])\n      2 df[\"target\"] = digits.values\n----&gt; 3 sns.scatterplot(x=\"x\", y=\"y\", c=\"target\")\n      4 #model.plot.points(model, labels=digits.target, width=400, height=400)\n\nFile ~/miniconda3/lib/python3.11/site-packages/seaborn/relational.py:615, in scatterplot(data, x, y, hue, size, style, palette, hue_order, hue_norm, sizes, size_order, size_norm, markers, style_order, legend, ax, **kwargs)\n    606 def scatterplot(\n    607     data=None, *,\n    608     x=None, y=None, hue=None, size=None, style=None,\n   (...)\n    612     **kwargs\n    613 ):\n--&gt; 615     p = _ScatterPlotter(\n    616         data=data,\n    617         variables=dict(x=x, y=y, hue=hue, size=size, style=style),\n    618         legend=legend\n    619     )\n    621     p.map_hue(palette=palette, order=hue_order, norm=hue_norm)\n    622     p.map_size(sizes=sizes, order=size_order, norm=size_norm)\n\nFile ~/miniconda3/lib/python3.11/site-packages/seaborn/relational.py:396, in _ScatterPlotter.__init__(self, data, variables, legend)\n    387 def __init__(self, *, data=None, variables={}, legend=None):\n    388 \n    389     # TODO this is messy, we want the mapping to be agnostic about\n    390     # the kind of plot to draw, but for the time being we need to set\n    391     # this information so the SizeMapping can use it\n    392     self._default_size_range = (\n    393         np.r_[.5, 2] * np.square(mpl.rcParams[\"lines.markersize\"])\n    394     )\n--&gt; 396     super().__init__(data=data, variables=variables)\n    398     self.legend = legend\n\nFile ~/miniconda3/lib/python3.11/site-packages/seaborn/_base.py:634, in VectorPlotter.__init__(self, data, variables)\n    629 # var_ordered is relevant only for categorical axis variables, and may\n    630 # be better handled by an internal axis information object that tracks\n    631 # such information and is set up by the scale_* methods. The analogous\n    632 # information for numeric axes would be information about log scales.\n    633 self._var_ordered = {\"x\": False, \"y\": False}  # alt., used DefaultDict\n--&gt; 634 self.assign_variables(data, variables)\n    636 # TODO Lots of tests assume that these are called to initialize the\n    637 # mappings to default values on class initialization. I'd prefer to\n    638 # move away from that and only have a mapping when explicitly called.\n    639 for var in [\"hue\", \"size\", \"style\"]:\n\nFile ~/miniconda3/lib/python3.11/site-packages/seaborn/_base.py:679, in VectorPlotter.assign_variables(self, data, variables)\n    674 else:\n    675     # When dealing with long-form input, use the newer PlotData\n    676     # object (internal but introduced for the objects interface)\n    677     # to centralize / standardize data consumption logic.\n    678     self.input_format = \"long\"\n--&gt; 679     plot_data = PlotData(data, variables)\n    680     frame = plot_data.frame\n    681     names = plot_data.names\n\nFile ~/miniconda3/lib/python3.11/site-packages/seaborn/_core/data.py:58, in PlotData.__init__(self, data, variables)\n     51 def __init__(\n     52     self,\n     53     data: DataSource,\n     54     variables: dict[str, VariableSpec],\n     55 ):\n     57     data = handle_data_source(data)\n---&gt; 58     frame, names, ids = self._assign_variables(data, variables)\n     60     self.frame = frame\n     61     self.names = names\n\nFile ~/miniconda3/lib/python3.11/site-packages/seaborn/_core/data.py:232, in PlotData._assign_variables(self, data, variables)\n    230     else:\n    231         err += \"An entry with this name does not appear in `data`.\"\n--&gt; 232     raise ValueError(err)\n    234 else:\n    235 \n    236     # Otherwise, assume the value somehow represents data\n    237 \n    238     # Ignore empty data structures\n    239     if isinstance(val, Sized) and len(val) == 0:\n\nValueError: Could not interpret value `x` for `x`. Value is a string, but `data` was not passed.\n\n\n\n\ndf = pd.DataFrame(data=emb, columns=[\"x\", \"y\"])\ndf[\"target\"] = digits.values\nsns.scatterplot(x=\"x\", y=\"y\", c=\"target\", data=df)\n#model.plot.points(model, labels=digits.target, width=400, height=400)\n\n\n---------------------------------------------------------------------------\nValueError                                Traceback (most recent call last)\nFile ~/miniconda3/lib/python3.11/site-packages/matplotlib/axes/_axes.py:4618, in Axes._parse_scatter_color_args(c, edgecolors, kwargs, xsize, get_next_color_func)\n   4617 try:  # Is 'c' acceptable as PathCollection facecolors?\n-&gt; 4618     colors = mcolors.to_rgba_array(c)\n   4619 except (TypeError, ValueError) as err:\n\nFile ~/miniconda3/lib/python3.11/site-packages/matplotlib/colors.py:496, in to_rgba_array(c, alpha)\n    495 if isinstance(c, str):\n--&gt; 496     raise ValueError(f\"{c!r} is not a valid color value.\")\n    498 if len(c) == 0:\n\nValueError: 'target' is not a valid color value.\n\nThe above exception was the direct cause of the following exception:\n\nValueError                                Traceback (most recent call last)\nFile /home/sadit/Cursos/MEIA2025/umap.qmd:3\n      1 df = pd.DataFrame(data=emb, columns=[\"x\", \"y\"])\n      2 df[\"target\"] = digits.values\n----&gt; 3 sns.scatterplot(x=\"x\", y=\"y\", c=\"target\", data=df)\n      4 #model.plot.points(model, labels=digits.target, width=400, height=400)\n\nFile ~/miniconda3/lib/python3.11/site-packages/seaborn/relational.py:634, in scatterplot(data, x, y, hue, size, style, palette, hue_order, hue_norm, sizes, size_order, size_norm, markers, style_order, legend, ax, **kwargs)\n    631 p._attach(ax)\n    633 color = kwargs.pop(\"color\", None)\n--&gt; 634 kwargs[\"color\"] = _default_color(ax.scatter, hue, color, kwargs)\n    636 p.plot(ax, kwargs)\n    638 return ax\n\nFile ~/miniconda3/lib/python3.11/site-packages/seaborn/utils.py:105, in _default_color(method, hue, color, kws, saturation)\n     99 scout_size = max(\n    100     np.atleast_1d(kws.get(key, [])).shape[0]\n    101     for key in [\"s\", \"c\", \"fc\", \"facecolor\", \"facecolors\"]\n    102 )\n    103 scout_x = scout_y = np.full(scout_size, np.nan)\n--&gt; 105 scout = method(scout_x, scout_y, **kws)\n    106 facecolors = scout.get_facecolors()\n    108 if not len(facecolors):\n    109     # Handle bug in matplotlib &lt;= 3.2 (I think)\n    110     # This will limit the ability to use non color= kwargs to specify\n    111     # a color in versions of matplotlib with the bug, but trying to\n    112     # work out what the user wanted by re-implementing the broken logic\n    113     # of inspecting the kwargs is probably too brittle.\n\nFile ~/miniconda3/lib/python3.11/site-packages/matplotlib/__init__.py:1473, in _preprocess_data.&lt;locals&gt;.inner(ax, data, *args, **kwargs)\n   1470 @functools.wraps(func)\n   1471 def inner(ax, *args, data=None, **kwargs):\n   1472     if data is None:\n-&gt; 1473         return func(\n   1474             ax,\n   1475             *map(sanitize_sequence, args),\n   1476             **{k: sanitize_sequence(v) for k, v in kwargs.items()})\n   1478     bound = new_sig.bind(ax, *args, **kwargs)\n   1479     auto_label = (bound.arguments.get(label_namer)\n   1480                   or bound.kwargs.get(label_namer))\n\nFile ~/miniconda3/lib/python3.11/site-packages/matplotlib/axes/_axes.py:4805, in Axes.scatter(self, x, y, s, c, marker, cmap, norm, vmin, vmax, alpha, linewidths, edgecolors, plotnonfinite, **kwargs)\n   4802 if edgecolors is None:\n   4803     orig_edgecolor = kwargs.get('edgecolor', None)\n   4804 c, colors, edgecolors = \\\n-&gt; 4805     self._parse_scatter_color_args(\n   4806         c, edgecolors, kwargs, x.size,\n   4807         get_next_color_func=self._get_patches_for_fill.get_next_color)\n   4809 if plotnonfinite and colors is None:\n   4810     c = np.ma.masked_invalid(c)\n\nFile ~/miniconda3/lib/python3.11/site-packages/matplotlib/axes/_axes.py:4627, in Axes._parse_scatter_color_args(c, edgecolors, kwargs, xsize, get_next_color_func)\n   4624             raise invalid_shape_exception(c.size, xsize) from err\n   4625         # Both the mapping *and* the RGBA conversion failed: pretty\n   4626         # severe failure =&gt; one may appreciate a verbose feedback.\n-&gt; 4627         raise ValueError(\n   4628             f\"'c' argument must be a color, a sequence of colors, \"\n   4629             f\"or a sequence of numbers, not {c!r}\") from err\n   4630 else:\n   4631     if len(colors) not in (0, 1, xsize):\n   4632         # NB: remember that a single color is also acceptable.\n   4633         # Besides *colors* will be an empty array if c == 'none'.\n\nValueError: 'c' argument must be a color, a sequence of colors, or a sequence of numbers, not 'target'\n\n\n\n\n\n\n\n\n\n\n\ndf = pd.DataFrame(data=emb, columns=[\"x\", \"y\"])\ndf[\"target\"] = digits.values\nsns.scatterplot(x=\"x\", y=\"y\", color=\"target\", data=df)\n#model.plot.points(model, labels=digits.target, width=400, height=400)\n\n\n---------------------------------------------------------------------------\nValueError                                Traceback (most recent call last)\nFile ~/miniconda3/lib/python3.11/site-packages/matplotlib/axes/_axes.py:4557, in Axes._parse_scatter_color_args(c, edgecolors, kwargs, xsize, get_next_color_func)\n   4556 try:\n-&gt; 4557     mcolors.to_rgba_array(kwcolor)\n   4558 except ValueError as err:\n\nFile ~/miniconda3/lib/python3.11/site-packages/matplotlib/colors.py:496, in to_rgba_array(c, alpha)\n    495 if isinstance(c, str):\n--&gt; 496     raise ValueError(f\"{c!r} is not a valid color value.\")\n    498 if len(c) == 0:\n\nValueError: 'target' is not a valid color value.\n\nThe above exception was the direct cause of the following exception:\n\nValueError                                Traceback (most recent call last)\nFile /home/sadit/Cursos/MEIA2025/umap.qmd:3\n      1 df = pd.DataFrame(data=emb, columns=[\"x\", \"y\"])\n      2 df[\"target\"] = digits.values\n----&gt; 3 sns.scatterplot(x=\"x\", y=\"y\", color=\"target\", data=df)\n      4 #model.plot.points(model, labels=digits.target, width=400, height=400)\n\nFile ~/miniconda3/lib/python3.11/site-packages/seaborn/relational.py:636, in scatterplot(data, x, y, hue, size, style, palette, hue_order, hue_norm, sizes, size_order, size_norm, markers, style_order, legend, ax, **kwargs)\n    633 color = kwargs.pop(\"color\", None)\n    634 kwargs[\"color\"] = _default_color(ax.scatter, hue, color, kwargs)\n--&gt; 636 p.plot(ax, kwargs)\n    638 return ax\n\nFile ~/miniconda3/lib/python3.11/site-packages/seaborn/relational.py:438, in _ScatterPlotter.plot(self, ax, kws)\n    435     kws.setdefault(\"edgecolor\", \"w\")\n    437 # Draw the scatter plot\n--&gt; 438 points = ax.scatter(x=x, y=y, **kws)\n    440 # Apply the mapping from semantic variables to artist attributes\n    442 if \"hue\" in self.variables:\n\nFile ~/miniconda3/lib/python3.11/site-packages/matplotlib/__init__.py:1473, in _preprocess_data.&lt;locals&gt;.inner(ax, data, *args, **kwargs)\n   1470 @functools.wraps(func)\n   1471 def inner(ax, *args, data=None, **kwargs):\n   1472     if data is None:\n-&gt; 1473         return func(\n   1474             ax,\n   1475             *map(sanitize_sequence, args),\n   1476             **{k: sanitize_sequence(v) for k, v in kwargs.items()})\n   1478     bound = new_sig.bind(ax, *args, **kwargs)\n   1479     auto_label = (bound.arguments.get(label_namer)\n   1480                   or bound.kwargs.get(label_namer))\n\nFile ~/miniconda3/lib/python3.11/site-packages/matplotlib/axes/_axes.py:4805, in Axes.scatter(self, x, y, s, c, marker, cmap, norm, vmin, vmax, alpha, linewidths, edgecolors, plotnonfinite, **kwargs)\n   4802 if edgecolors is None:\n   4803     orig_edgecolor = kwargs.get('edgecolor', None)\n   4804 c, colors, edgecolors = \\\n-&gt; 4805     self._parse_scatter_color_args(\n   4806         c, edgecolors, kwargs, x.size,\n   4807         get_next_color_func=self._get_patches_for_fill.get_next_color)\n   4809 if plotnonfinite and colors is None:\n   4810     c = np.ma.masked_invalid(c)\n\nFile ~/miniconda3/lib/python3.11/site-packages/matplotlib/axes/_axes.py:4559, in Axes._parse_scatter_color_args(c, edgecolors, kwargs, xsize, get_next_color_func)\n   4557     mcolors.to_rgba_array(kwcolor)\n   4558 except ValueError as err:\n-&gt; 4559     raise ValueError(\n   4560         \"'color' kwarg must be a color or sequence of color \"\n   4561         \"specs.  For a sequence of values to be color-mapped, use \"\n   4562         \"the 'c' argument instead.\") from err\n   4563 if edgecolors is None:\n   4564     edgecolors = kwcolor\n\nValueError: 'color' kwarg must be a color or sequence of color specs.  For a sequence of values to be color-mapped, use the 'c' argument instead.\n\n\n\n\n\n\n\n\n\n\n\ndf = pd.DataFrame(data=emb, columns=[\"x\", \"y\"])\ndf[\"target\"] = digits.values\nsns.scatterplot(x=\"x\", y=\"y\", c=\"target\", data=df)\n#model.plot.points(model, labels=digits.target, width=400, height=400)\n\n\n---------------------------------------------------------------------------\nValueError                                Traceback (most recent call last)\nFile ~/miniconda3/lib/python3.11/site-packages/matplotlib/axes/_axes.py:4618, in Axes._parse_scatter_color_args(c, edgecolors, kwargs, xsize, get_next_color_func)\n   4617 try:  # Is 'c' acceptable as PathCollection facecolors?\n-&gt; 4618     colors = mcolors.to_rgba_array(c)\n   4619 except (TypeError, ValueError) as err:\n\nFile ~/miniconda3/lib/python3.11/site-packages/matplotlib/colors.py:496, in to_rgba_array(c, alpha)\n    495 if isinstance(c, str):\n--&gt; 496     raise ValueError(f\"{c!r} is not a valid color value.\")\n    498 if len(c) == 0:\n\nValueError: 'target' is not a valid color value.\n\nThe above exception was the direct cause of the following exception:\n\nValueError                                Traceback (most recent call last)\nFile /home/sadit/Cursos/MEIA2025/umap.qmd:3\n      1 df = pd.DataFrame(data=emb, columns=[\"x\", \"y\"])\n      2 df[\"target\"] = digits.values\n----&gt; 3 sns.scatterplot(x=\"x\", y=\"y\", c=\"target\", data=df)\n      4 #model.plot.points(model, labels=digits.target, width=400, height=400)\n\nFile ~/miniconda3/lib/python3.11/site-packages/seaborn/relational.py:634, in scatterplot(data, x, y, hue, size, style, palette, hue_order, hue_norm, sizes, size_order, size_norm, markers, style_order, legend, ax, **kwargs)\n    631 p._attach(ax)\n    633 color = kwargs.pop(\"color\", None)\n--&gt; 634 kwargs[\"color\"] = _default_color(ax.scatter, hue, color, kwargs)\n    636 p.plot(ax, kwargs)\n    638 return ax\n\nFile ~/miniconda3/lib/python3.11/site-packages/seaborn/utils.py:105, in _default_color(method, hue, color, kws, saturation)\n     99 scout_size = max(\n    100     np.atleast_1d(kws.get(key, [])).shape[0]\n    101     for key in [\"s\", \"c\", \"fc\", \"facecolor\", \"facecolors\"]\n    102 )\n    103 scout_x = scout_y = np.full(scout_size, np.nan)\n--&gt; 105 scout = method(scout_x, scout_y, **kws)\n    106 facecolors = scout.get_facecolors()\n    108 if not len(facecolors):\n    109     # Handle bug in matplotlib &lt;= 3.2 (I think)\n    110     # This will limit the ability to use non color= kwargs to specify\n    111     # a color in versions of matplotlib with the bug, but trying to\n    112     # work out what the user wanted by re-implementing the broken logic\n    113     # of inspecting the kwargs is probably too brittle.\n\nFile ~/miniconda3/lib/python3.11/site-packages/matplotlib/__init__.py:1473, in _preprocess_data.&lt;locals&gt;.inner(ax, data, *args, **kwargs)\n   1470 @functools.wraps(func)\n   1471 def inner(ax, *args, data=None, **kwargs):\n   1472     if data is None:\n-&gt; 1473         return func(\n   1474             ax,\n   1475             *map(sanitize_sequence, args),\n   1476             **{k: sanitize_sequence(v) for k, v in kwargs.items()})\n   1478     bound = new_sig.bind(ax, *args, **kwargs)\n   1479     auto_label = (bound.arguments.get(label_namer)\n   1480                   or bound.kwargs.get(label_namer))\n\nFile ~/miniconda3/lib/python3.11/site-packages/matplotlib/axes/_axes.py:4805, in Axes.scatter(self, x, y, s, c, marker, cmap, norm, vmin, vmax, alpha, linewidths, edgecolors, plotnonfinite, **kwargs)\n   4802 if edgecolors is None:\n   4803     orig_edgecolor = kwargs.get('edgecolor', None)\n   4804 c, colors, edgecolors = \\\n-&gt; 4805     self._parse_scatter_color_args(\n   4806         c, edgecolors, kwargs, x.size,\n   4807         get_next_color_func=self._get_patches_for_fill.get_next_color)\n   4809 if plotnonfinite and colors is None:\n   4810     c = np.ma.masked_invalid(c)\n\nFile ~/miniconda3/lib/python3.11/site-packages/matplotlib/axes/_axes.py:4627, in Axes._parse_scatter_color_args(c, edgecolors, kwargs, xsize, get_next_color_func)\n   4624             raise invalid_shape_exception(c.size, xsize) from err\n   4625         # Both the mapping *and* the RGBA conversion failed: pretty\n   4626         # severe failure =&gt; one may appreciate a verbose feedback.\n-&gt; 4627         raise ValueError(\n   4628             f\"'c' argument must be a color, a sequence of colors, \"\n   4629             f\"or a sequence of numbers, not {c!r}\") from err\n   4630 else:\n   4631     if len(colors) not in (0, 1, xsize):\n   4632         # NB: remember that a single color is also acceptable.\n   4633         # Besides *colors* will be an empty array if c == 'none'.\n\nValueError: 'c' argument must be a color, a sequence of colors, or a sequence of numbers, not 'target'\n\n\n\n\n\n\n\n\n\n\n\ndigits.values\n\n&lt;function Bunch.values&gt;\n\n\n\ndigits.images\n\narray([[[ 0.,  0.,  5., ...,  1.,  0.,  0.],\n        [ 0.,  0., 13., ..., 15.,  5.,  0.],\n        [ 0.,  3., 15., ..., 11.,  8.,  0.],\n        ...,\n        [ 0.,  4., 11., ..., 12.,  7.,  0.],\n        [ 0.,  2., 14., ..., 12.,  0.,  0.],\n        [ 0.,  0.,  6., ...,  0.,  0.,  0.]],\n\n       [[ 0.,  0.,  0., ...,  5.,  0.,  0.],\n        [ 0.,  0.,  0., ...,  9.,  0.,  0.],\n        [ 0.,  0.,  3., ...,  6.,  0.,  0.],\n        ...,\n        [ 0.,  0.,  1., ...,  6.,  0.,  0.],\n        [ 0.,  0.,  1., ...,  6.,  0.,  0.],\n        [ 0.,  0.,  0., ..., 10.,  0.,  0.]],\n\n       [[ 0.,  0.,  0., ..., 12.,  0.,  0.],\n        [ 0.,  0.,  3., ..., 14.,  0.,  0.],\n        [ 0.,  0.,  8., ..., 16.,  0.,  0.],\n        ...,\n        [ 0.,  9., 16., ...,  0.,  0.,  0.],\n        [ 0.,  3., 13., ..., 11.,  5.,  0.],\n        [ 0.,  0.,  0., ..., 16.,  9.,  0.]],\n\n       ...,\n\n       [[ 0.,  0.,  1., ...,  1.,  0.,  0.],\n        [ 0.,  0., 13., ...,  2.,  1.,  0.],\n        [ 0.,  0., 16., ..., 16.,  5.,  0.],\n        ...,\n        [ 0.,  0., 16., ..., 15.,  0.,  0.],\n        [ 0.,  0., 15., ..., 16.,  0.,  0.],\n        [ 0.,  0.,  2., ...,  6.,  0.,  0.]],\n\n       [[ 0.,  0.,  2., ...,  0.,  0.,  0.],\n        [ 0.,  0., 14., ..., 15.,  1.,  0.],\n        [ 0.,  4., 16., ..., 16.,  7.,  0.],\n        ...,\n        [ 0.,  0.,  0., ..., 16.,  2.,  0.],\n        [ 0.,  0.,  4., ..., 16.,  2.,  0.],\n        [ 0.,  0.,  5., ..., 12.,  0.,  0.]],\n\n       [[ 0.,  0., 10., ...,  1.,  0.,  0.],\n        [ 0.,  2., 16., ...,  1.,  0.,  0.],\n        [ 0.,  0., 15., ..., 15.,  0.,  0.],\n        ...,\n        [ 0.,  4., 16., ..., 16.,  6.,  0.],\n        [ 0.,  8., 16., ..., 16.,  8.,  0.],\n        [ 0.,  1.,  8., ..., 12.,  1.,  0.]]])\n\n\n\ndigits.DESCR\n\n\".. _digits_dataset:\\n\\nOptical recognition of handwritten digits dataset\\n--------------------------------------------------\\n\\n**Data Set Characteristics:**\\n\\n    :Number of Instances: 1797\\n    :Number of Attributes: 64\\n    :Attribute Information: 8x8 image of integer pixels in the range 0..16.\\n    :Missing Attribute Values: None\\n    :Creator: E. Alpaydin (alpaydin '@' boun.edu.tr)\\n    :Date: July; 1998\\n\\nThis is a copy of the test set of the UCI ML hand-written digits datasets\\nhttps://archive.ics.uci.edu/ml/datasets/Optical+Recognition+of+Handwritten+Digits\\n\\nThe data set contains images of hand-written digits: 10 classes where\\neach class refers to a digit.\\n\\nPreprocessing programs made available by NIST were used to extract\\nnormalized bitmaps of handwritten digits from a preprinted form. From a\\ntotal of 43 people, 30 contributed to the training set and different 13\\nto the test set. 32x32 bitmaps are divided into nonoverlapping blocks of\\n4x4 and the number of on pixels are counted in each block. This generates\\nan input matrix of 8x8 where each element is an integer in the range\\n0..16. This reduces dimensionality and gives invariance to small\\ndistortions.\\n\\nFor info on NIST preprocessing routines, see M. D. Garris, J. L. Blue, G.\\nT. Candela, D. L. Dimmick, J. Geist, P. J. Grother, S. A. Janet, and C.\\nL. Wilson, NIST Form-Based Handprint Recognition System, NISTIR 5469,\\n1994.\\n\\n|details-start|\\n**References**\\n|details-split|\\n\\n- C. Kaynak (1995) Methods of Combining Multiple Classifiers and Their\\n  Applications to Handwritten Digit Recognition, MSc Thesis, Institute of\\n  Graduate Studies in Science and Engineering, Bogazici University.\\n- E. Alpaydin, C. Kaynak (1998) Cascading Classifiers, Kybernetika.\\n- Ken Tang and Ponnuthurai N. Suganthan and Xi Yao and A. Kai Qin.\\n  Linear dimensionalityreduction using relevance weighted LDA. School of\\n  Electrical and Electronic Engineering Nanyang Technological University.\\n  2005.\\n- Claudio Gentile. A New Approximate Maximal Margin Classification\\n  Algorithm. NIPS. 2000.\\n\\n|details-end|\"\n\n\n\ndigits.feature_names\n\n['pixel_0_0',\n 'pixel_0_1',\n 'pixel_0_2',\n 'pixel_0_3',\n 'pixel_0_4',\n 'pixel_0_5',\n 'pixel_0_6',\n 'pixel_0_7',\n 'pixel_1_0',\n 'pixel_1_1',\n 'pixel_1_2',\n 'pixel_1_3',\n 'pixel_1_4',\n 'pixel_1_5',\n 'pixel_1_6',\n 'pixel_1_7',\n 'pixel_2_0',\n 'pixel_2_1',\n 'pixel_2_2',\n 'pixel_2_3',\n 'pixel_2_4',\n 'pixel_2_5',\n 'pixel_2_6',\n 'pixel_2_7',\n 'pixel_3_0',\n 'pixel_3_1',\n 'pixel_3_2',\n 'pixel_3_3',\n 'pixel_3_4',\n 'pixel_3_5',\n 'pixel_3_6',\n 'pixel_3_7',\n 'pixel_4_0',\n 'pixel_4_1',\n 'pixel_4_2',\n 'pixel_4_3',\n 'pixel_4_4',\n 'pixel_4_5',\n 'pixel_4_6',\n 'pixel_4_7',\n 'pixel_5_0',\n 'pixel_5_1',\n 'pixel_5_2',\n 'pixel_5_3',\n 'pixel_5_4',\n 'pixel_5_5',\n 'pixel_5_6',\n 'pixel_5_7',\n 'pixel_6_0',\n 'pixel_6_1',\n 'pixel_6_2',\n 'pixel_6_3',\n 'pixel_6_4',\n 'pixel_6_5',\n 'pixel_6_6',\n 'pixel_6_7',\n 'pixel_7_0',\n 'pixel_7_1',\n 'pixel_7_2',\n 'pixel_7_3',\n 'pixel_7_4',\n 'pixel_7_5',\n 'pixel_7_6',\n 'pixel_7_7']\n\n\n\ndigits.target\n\narray([0, 1, 2, ..., 8, 9, 8])\n\n\n\ndf = pd.DataFrame(data=emb, columns=[\"x\", \"y\"])\ndf[\"target\"] = digits.target\nsns.scatterplot(x=\"x\", y=\"y\", c=\"target\", data=df)\n#model.plot.points(model, labels=digits.target, width=400, height=400)\n\n\n---------------------------------------------------------------------------\nValueError                                Traceback (most recent call last)\nFile ~/miniconda3/lib/python3.11/site-packages/matplotlib/axes/_axes.py:4618, in Axes._parse_scatter_color_args(c, edgecolors, kwargs, xsize, get_next_color_func)\n   4617 try:  # Is 'c' acceptable as PathCollection facecolors?\n-&gt; 4618     colors = mcolors.to_rgba_array(c)\n   4619 except (TypeError, ValueError) as err:\n\nFile ~/miniconda3/lib/python3.11/site-packages/matplotlib/colors.py:496, in to_rgba_array(c, alpha)\n    495 if isinstance(c, str):\n--&gt; 496     raise ValueError(f\"{c!r} is not a valid color value.\")\n    498 if len(c) == 0:\n\nValueError: 'target' is not a valid color value.\n\nThe above exception was the direct cause of the following exception:\n\nValueError                                Traceback (most recent call last)\nFile /home/sadit/Cursos/MEIA2025/umap.qmd:3\n      1 df = pd.DataFrame(data=emb, columns=[\"x\", \"y\"])\n      2 df[\"target\"] = digits.target\n----&gt; 3 sns.scatterplot(x=\"x\", y=\"y\", c=\"target\", data=df)\n      4 #model.plot.points(model, labels=digits.target, width=400, height=400)\n\nFile ~/miniconda3/lib/python3.11/site-packages/seaborn/relational.py:634, in scatterplot(data, x, y, hue, size, style, palette, hue_order, hue_norm, sizes, size_order, size_norm, markers, style_order, legend, ax, **kwargs)\n    631 p._attach(ax)\n    633 color = kwargs.pop(\"color\", None)\n--&gt; 634 kwargs[\"color\"] = _default_color(ax.scatter, hue, color, kwargs)\n    636 p.plot(ax, kwargs)\n    638 return ax\n\nFile ~/miniconda3/lib/python3.11/site-packages/seaborn/utils.py:105, in _default_color(method, hue, color, kws, saturation)\n     99 scout_size = max(\n    100     np.atleast_1d(kws.get(key, [])).shape[0]\n    101     for key in [\"s\", \"c\", \"fc\", \"facecolor\", \"facecolors\"]\n    102 )\n    103 scout_x = scout_y = np.full(scout_size, np.nan)\n--&gt; 105 scout = method(scout_x, scout_y, **kws)\n    106 facecolors = scout.get_facecolors()\n    108 if not len(facecolors):\n    109     # Handle bug in matplotlib &lt;= 3.2 (I think)\n    110     # This will limit the ability to use non color= kwargs to specify\n    111     # a color in versions of matplotlib with the bug, but trying to\n    112     # work out what the user wanted by re-implementing the broken logic\n    113     # of inspecting the kwargs is probably too brittle.\n\nFile ~/miniconda3/lib/python3.11/site-packages/matplotlib/__init__.py:1473, in _preprocess_data.&lt;locals&gt;.inner(ax, data, *args, **kwargs)\n   1470 @functools.wraps(func)\n   1471 def inner(ax, *args, data=None, **kwargs):\n   1472     if data is None:\n-&gt; 1473         return func(\n   1474             ax,\n   1475             *map(sanitize_sequence, args),\n   1476             **{k: sanitize_sequence(v) for k, v in kwargs.items()})\n   1478     bound = new_sig.bind(ax, *args, **kwargs)\n   1479     auto_label = (bound.arguments.get(label_namer)\n   1480                   or bound.kwargs.get(label_namer))\n\nFile ~/miniconda3/lib/python3.11/site-packages/matplotlib/axes/_axes.py:4805, in Axes.scatter(self, x, y, s, c, marker, cmap, norm, vmin, vmax, alpha, linewidths, edgecolors, plotnonfinite, **kwargs)\n   4802 if edgecolors is None:\n   4803     orig_edgecolor = kwargs.get('edgecolor', None)\n   4804 c, colors, edgecolors = \\\n-&gt; 4805     self._parse_scatter_color_args(\n   4806         c, edgecolors, kwargs, x.size,\n   4807         get_next_color_func=self._get_patches_for_fill.get_next_color)\n   4809 if plotnonfinite and colors is None:\n   4810     c = np.ma.masked_invalid(c)\n\nFile ~/miniconda3/lib/python3.11/site-packages/matplotlib/axes/_axes.py:4627, in Axes._parse_scatter_color_args(c, edgecolors, kwargs, xsize, get_next_color_func)\n   4624             raise invalid_shape_exception(c.size, xsize) from err\n   4625         # Both the mapping *and* the RGBA conversion failed: pretty\n   4626         # severe failure =&gt; one may appreciate a verbose feedback.\n-&gt; 4627         raise ValueError(\n   4628             f\"'c' argument must be a color, a sequence of colors, \"\n   4629             f\"or a sequence of numbers, not {c!r}\") from err\n   4630 else:\n   4631     if len(colors) not in (0, 1, xsize):\n   4632         # NB: remember that a single color is also acceptable.\n   4633         # Besides *colors* will be an empty array if c == 'none'.\n\nValueError: 'c' argument must be a color, a sequence of colors, or a sequence of numbers, not 'target'\n\n\n\n\n\n\n\n\n\n\n\ndf = pd.DataFrame(data=emb, columns=[\"x\", \"y\"])\ndf[\"target\"] = digits.target\nsns.scatterplot(x=\"x\", y=\"y\", c=\"target\", data=df)\n#model.plot.points(model, labels=digits.target, width=400, height=400)\n\n\n---------------------------------------------------------------------------\nValueError                                Traceback (most recent call last)\nFile ~/miniconda3/lib/python3.11/site-packages/matplotlib/axes/_axes.py:4618, in Axes._parse_scatter_color_args(c, edgecolors, kwargs, xsize, get_next_color_func)\n   4617 try:  # Is 'c' acceptable as PathCollection facecolors?\n-&gt; 4618     colors = mcolors.to_rgba_array(c)\n   4619 except (TypeError, ValueError) as err:\n\nFile ~/miniconda3/lib/python3.11/site-packages/matplotlib/colors.py:496, in to_rgba_array(c, alpha)\n    495 if isinstance(c, str):\n--&gt; 496     raise ValueError(f\"{c!r} is not a valid color value.\")\n    498 if len(c) == 0:\n\nValueError: 'target' is not a valid color value.\n\nThe above exception was the direct cause of the following exception:\n\nValueError                                Traceback (most recent call last)\nFile /home/sadit/Cursos/MEIA2025/umap.qmd:3\n      1 df = pd.DataFrame(data=emb, columns=[\"x\", \"y\"])\n      2 df[\"target\"] = digits.target\n----&gt; 3 sns.scatterplot(x=\"x\", y=\"y\", c=\"target\", data=df)\n      4 #model.plot.points(model, labels=digits.target, width=400, height=400)\n\nFile ~/miniconda3/lib/python3.11/site-packages/seaborn/relational.py:634, in scatterplot(data, x, y, hue, size, style, palette, hue_order, hue_norm, sizes, size_order, size_norm, markers, style_order, legend, ax, **kwargs)\n    631 p._attach(ax)\n    633 color = kwargs.pop(\"color\", None)\n--&gt; 634 kwargs[\"color\"] = _default_color(ax.scatter, hue, color, kwargs)\n    636 p.plot(ax, kwargs)\n    638 return ax\n\nFile ~/miniconda3/lib/python3.11/site-packages/seaborn/utils.py:105, in _default_color(method, hue, color, kws, saturation)\n     99 scout_size = max(\n    100     np.atleast_1d(kws.get(key, [])).shape[0]\n    101     for key in [\"s\", \"c\", \"fc\", \"facecolor\", \"facecolors\"]\n    102 )\n    103 scout_x = scout_y = np.full(scout_size, np.nan)\n--&gt; 105 scout = method(scout_x, scout_y, **kws)\n    106 facecolors = scout.get_facecolors()\n    108 if not len(facecolors):\n    109     # Handle bug in matplotlib &lt;= 3.2 (I think)\n    110     # This will limit the ability to use non color= kwargs to specify\n    111     # a color in versions of matplotlib with the bug, but trying to\n    112     # work out what the user wanted by re-implementing the broken logic\n    113     # of inspecting the kwargs is probably too brittle.\n\nFile ~/miniconda3/lib/python3.11/site-packages/matplotlib/__init__.py:1473, in _preprocess_data.&lt;locals&gt;.inner(ax, data, *args, **kwargs)\n   1470 @functools.wraps(func)\n   1471 def inner(ax, *args, data=None, **kwargs):\n   1472     if data is None:\n-&gt; 1473         return func(\n   1474             ax,\n   1475             *map(sanitize_sequence, args),\n   1476             **{k: sanitize_sequence(v) for k, v in kwargs.items()})\n   1478     bound = new_sig.bind(ax, *args, **kwargs)\n   1479     auto_label = (bound.arguments.get(label_namer)\n   1480                   or bound.kwargs.get(label_namer))\n\nFile ~/miniconda3/lib/python3.11/site-packages/matplotlib/axes/_axes.py:4805, in Axes.scatter(self, x, y, s, c, marker, cmap, norm, vmin, vmax, alpha, linewidths, edgecolors, plotnonfinite, **kwargs)\n   4802 if edgecolors is None:\n   4803     orig_edgecolor = kwargs.get('edgecolor', None)\n   4804 c, colors, edgecolors = \\\n-&gt; 4805     self._parse_scatter_color_args(\n   4806         c, edgecolors, kwargs, x.size,\n   4807         get_next_color_func=self._get_patches_for_fill.get_next_color)\n   4809 if plotnonfinite and colors is None:\n   4810     c = np.ma.masked_invalid(c)\n\nFile ~/miniconda3/lib/python3.11/site-packages/matplotlib/axes/_axes.py:4627, in Axes._parse_scatter_color_args(c, edgecolors, kwargs, xsize, get_next_color_func)\n   4624             raise invalid_shape_exception(c.size, xsize) from err\n   4625         # Both the mapping *and* the RGBA conversion failed: pretty\n   4626         # severe failure =&gt; one may appreciate a verbose feedback.\n-&gt; 4627         raise ValueError(\n   4628             f\"'c' argument must be a color, a sequence of colors, \"\n   4629             f\"or a sequence of numbers, not {c!r}\") from err\n   4630 else:\n   4631     if len(colors) not in (0, 1, xsize):\n   4632         # NB: remember that a single color is also acceptable.\n   4633         # Besides *colors* will be an empty array if c == 'none'.\n\nValueError: 'c' argument must be a color, a sequence of colors, or a sequence of numbers, not 'target'\n\n\n\n\n\n\n\n\n\n\n\ndf = pd.DataFrame(data=emb, columns=[\"x\", \"y\"])\ndf[\"target\"] = digits.target\nsns.scatterplot(x=\"x\", y=\"y\", c=df.target, data=df)\n#model.plot.points(model, labels=digits.target, width=400, height=400)\n\n\n\n\n\n\n\n\n\ndf = pd.DataFrame(data=emb, columns=[\"x\", \"y\"])\ndf[\"target\"] = digits.target\nsns.scatterplot(x=\"x\", y=\"y\", hue=\"target\", data=df)\n\n\n\n\n\n\n\n\n\ndf = pd.DataFrame(data=emb, columns=[\"x\", \"y\"])\ndf[\"target\"] = digits.target\nsns.scatterplot(x=\"x\", y=\"y\",\n    hue=\"target\",palette='viridis', # Esquema de colores\n    alpha=0.7, data=df)\n\n\n\n\n\n\n\n\n\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.decomposition import PCA\n\nscaler = StandardScaler()\nscaled_features = scaler.fit_transform(digits.data)\nmodel = PCA(n_components=2)\nprincipal_components = model.fit_transform(scaled_features)\npca_df = pd.DataFrame(data=principal_components, columns=['PC1', 'PC2'])\npca_df['target'] = digits['target']\nsns.scatterplot(\n    data=pca_df,\n    x='PC1',\n    y='PC2',\n    hue='target', # Colorea los puntos según si sobrevivieron o no\n    palette='viridis', # Esquema de colores\n    alpha=0.7 # Transparencia\n)\n\n\n\n\n\n\n\n\n\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.decomposition import PCA\n\n#scaler = StandardScaler()\n#scaled_features = scaler.fit_transform(digits.data)\nscaled_features = digits.data\nmodel = PCA(n_components=2)\nprincipal_components = model.fit_transform(scaled_features)\npca_df = pd.DataFrame(data=principal_components, columns=['PC1', 'PC2'])\npca_df['target'] = digits['target']\nsns.scatterplot(\n    data=pca_df,\n    x='PC1',\n    y='PC2',\n    hue='target', # Colorea los puntos según si sobrevivieron o no\n    palette='viridis', # Esquema de colores\n    alpha=0.7 # Transparencia\n)"
  }
]